{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Data Acquisition and Management with Advanced Web Scraping and AWS S3 Integration\n",
    "\n",
    "In the modern digital landscape, the ability to harness and manage vast amounts of data from various online sources is indispensable. This advanced tutorial guides you through the process of enhancing a basic web scraping tool to efficiently navigate and extract data from multiple pages of a website. It further elaborates on how to seamlessly integrate this data into Amazon S3, providing a robust solution for data storage and management.\n",
    "\n",
    "Detailed Explanation\n",
    "Web Scraping Enhancement\n",
    "Initially, the task involves upgrading an existing web scraping script. The script is set up to interact with a web browser programmatically, allowing it to navigate through a website and collect data from multiple pages automatically. This is particularly useful for websites with pagination, where data is spread across several pages. The script uses browser automation tools to load each page, extract necessary information, and handle transitions between pages smoothly.\n",
    "\n",
    "Data Compilation and Structuring\n",
    "As the data is scraped from each webpage, it is temporarily stored and then compiled into a comprehensive dataframe. This dataframe acts as a unified structure for all the collected data, making it easier to process and analyze. The structure includes various attributes such as organization names, registration numbers, and other pertinent details that are neatly organized into columns and rows.\n",
    "\n",
    "Secure and Efficient Data Upload to AWS S3\n",
    "Following data compilation, the focus shifts to securely uploading the dataset to Amazon Web Services (AWS) S3, a scalable object storage service. This section covers the verification of an existing storage bucket or the creation of a new one if necessary. The process ensures that all data is uploaded securely and efficiently, using proper authentication and error handling methods to prevent data loss and ensure integrity.\n",
    "\n",
    "Automation and Reliability\n",
    "The script is designed to automate the entire process, from data scraping to uploading, minimizing manual intervention and potential errors. This automation is crucial for tasks requiring regular data updates or handling large datasets. The integration with AWS S3 also provides a reliable and secure storage solution, making the data easily accessible for future analysis or backup purposes.\n",
    "\n",
    "Practical Implications\n",
    "This approach is invaluable for organizations that rely on continuous data updates from various sources. It can be utilized for market research, regulatory compliance, customer behavior analysis, and more. The technique ensures that data-driven decisions are based on the most current and comprehensive data available, thereby enhancing operational efficiency and strategic planning.\n",
    "\n",
    "In summary, this advanced method not only streamlines the data acquisition process through sophisticated web scraping techniques but also leverages cloud technology for optimal data storage and management, demonstrating a significant advancement in the field of data science and analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"94e0fb67d9b6ec8edd7d66a01915af7f\", element=\"b748c026-1a22-4a32-8995-ddefba1c67c8\")>\n"
     ]
    }
   ],
   "source": [
    "###Load modules\n",
    "import awscli\n",
    "import boto3\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "####SCRAPE THE WEBSITE######\n",
    "# create a new Chrome driver service\n",
    "chrome_driver_service = webdriver.chrome.service.Service('C:/Users/RAI/Desktop/Information-Architectures/chromedriver.exe')\n",
    "\n",
    "# start the service\n",
    "chrome_driver_service.start()\n",
    "\n",
    "# create a new Chrome browser instance using the service\n",
    "browser = webdriver.Chrome(service=chrome_driver_service)\n",
    "#enter the url path that needs to be accessed by webdriver\n",
    "browser.get('https://www.charitiesnys.com/RegistrySearch/search_charities.jsp')\n",
    "\n",
    "#identify xpath of location to select element\n",
    "inputElement = browser.find_element(By.XPATH,\"/html/body/div/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[2]/td[2]/input[1]\")\n",
    "inputElement.send_keys('0')\n",
    "inputElement1 = browser.find_element(By.XPATH,\"/html/body/div/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[10]/td/input[1]\").click()\n",
    "sleep(4) #allow for the page to load by adding a sleep element\n",
    "#identify the table to scrape\n",
    "table = browser.find_element(By.CSS_SELECTOR,'table.Bordered')\n",
    "sleep(1)\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organization Name</th>\n",
       "      <th>NY Reg #</th>\n",
       "      <th>EIN</th>\n",
       "      <th>Registrant Type</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Forever Captain Poodaman\" The Ahmad Butler Fo...</td>\n",
       "      <td>48-07-16</td>\n",
       "      <td>843800926</td>\n",
       "      <td>NFP</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"R\" S.U.C.C.E.S.S. Foundation Inc.</td>\n",
       "      <td>49-06-59</td>\n",
       "      <td>874012670</td>\n",
       "      <td>NFP</td>\n",
       "      <td>ROCHESTER</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Studio 5404\" Inc.</td>\n",
       "      <td>44-39-58</td>\n",
       "      <td>463180470</td>\n",
       "      <td>NFP</td>\n",
       "      <td>MASSAPAQUA</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"THEY ARE HAITIAN\" FUND, INC.</td>\n",
       "      <td>20-63-46</td>\n",
       "      <td>300170128</td>\n",
       "      <td>NFP</td>\n",
       "      <td>HUDSON</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Y\" Dive, Inc.</td>\n",
       "      <td>48-45-01</td>\n",
       "      <td>854252095</td>\n",
       "      <td>NFP</td>\n",
       "      <td>SAINT ALBANS</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(ASMA) American Syrian Multicultural Associati...</td>\n",
       "      <td>42-84-63</td>\n",
       "      <td>273130182</td>\n",
       "      <td>NFP</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#FeedHamburg</td>\n",
       "      <td>48-37-35</td>\n",
       "      <td>854150318</td>\n",
       "      <td>NFP</td>\n",
       "      <td>HAMBURG</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#HicksStrong Inc.</td>\n",
       "      <td>48-10-48</td>\n",
       "      <td>842612081</td>\n",
       "      <td>NFP</td>\n",
       "      <td>CLIFTON PARK</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#WalkAway Foundation</td>\n",
       "      <td>47-15-80</td>\n",
       "      <td>832820906</td>\n",
       "      <td>NFP</td>\n",
       "      <td>CARLSBAD</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>04/11 10:17 PM test</td>\n",
       "      <td>47-13-95</td>\n",
       "      <td>206256427</td>\n",
       "      <td>NFP</td>\n",
       "      <td>ALBANY</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1/20/21 Action Fund</td>\n",
       "      <td>46-99-13</td>\n",
       "      <td>832210730</td>\n",
       "      <td>NFP</td>\n",
       "      <td>SAN FRANCISCO</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10/40 Connections, Inc.</td>\n",
       "      <td>45-70-15</td>\n",
       "      <td>621825230</td>\n",
       "      <td>NFP</td>\n",
       "      <td>HIXSON</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1000 Feet Project, Inc</td>\n",
       "      <td>45-00-14</td>\n",
       "      <td>473820859</td>\n",
       "      <td>NFP</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1000 Islands Hose Haulers</td>\n",
       "      <td>45-38-38</td>\n",
       "      <td>454570241</td>\n",
       "      <td>NFP</td>\n",
       "      <td>CARTHAGE</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1004 Foundation, Inc.</td>\n",
       "      <td>45-00-13</td>\n",
       "      <td>463110658</td>\n",
       "      <td>NFP</td>\n",
       "      <td>LONG ISLAND CITY</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Organization Name  NY Reg #        EIN  \\\n",
       "0                                                None      None       None   \n",
       "1   \"Forever Captain Poodaman\" The Ahmad Butler Fo...  48-07-16  843800926   \n",
       "2                  \"R\" S.U.C.C.E.S.S. Foundation Inc.  49-06-59  874012670   \n",
       "3                                  \"Studio 5404\" Inc.  44-39-58  463180470   \n",
       "4                       \"THEY ARE HAITIAN\" FUND, INC.  20-63-46  300170128   \n",
       "5                                      \"Y\" Dive, Inc.  48-45-01  854252095   \n",
       "6   (ASMA) American Syrian Multicultural Associati...  42-84-63  273130182   \n",
       "7                                        #FeedHamburg  48-37-35  854150318   \n",
       "8                                   #HicksStrong Inc.  48-10-48  842612081   \n",
       "9                                #WalkAway Foundation  47-15-80  832820906   \n",
       "10                                04/11 10:17 PM test  47-13-95  206256427   \n",
       "11                                1/20/21 Action Fund  46-99-13  832210730   \n",
       "12                            10/40 Connections, Inc.  45-70-15  621825230   \n",
       "13                             1000 Feet Project, Inc  45-00-14  473820859   \n",
       "14                          1000 Islands Hose Haulers  45-38-38  454570241   \n",
       "15                              1004 Foundation, Inc.  45-00-13  463110658   \n",
       "\n",
       "   Registrant Type              City State  \n",
       "0             None              None  None  \n",
       "1              NFP      PHILADELPHIA    PA  \n",
       "2              NFP         ROCHESTER    NY  \n",
       "3              NFP        MASSAPAQUA    NY  \n",
       "4              NFP            HUDSON    NY  \n",
       "5              NFP      SAINT ALBANS    NY  \n",
       "6              NFP          BROOKLYN    NY  \n",
       "7              NFP           HAMBURG    NY  \n",
       "8              NFP      CLIFTON PARK    NY  \n",
       "9              NFP          CARLSBAD    CA  \n",
       "10             NFP            ALBANY    NY  \n",
       "11             NFP     SAN FRANCISCO    CA  \n",
       "12             NFP            HIXSON    TN  \n",
       "13             NFP          NEW YORK    NY  \n",
       "14             NFP          CARTHAGE    NY  \n",
       "15             NFP  LONG ISLAND CITY    NY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####CREATE DATE FRAME#####\n",
    "#create empty dataframe\n",
    "df =[]\n",
    "\n",
    "#loop through dataframe to export table\n",
    "for row in table.find_elements(By.CSS_SELECTOR,'tr'):\n",
    "      cols = df.append([cell.text for cell in row.find_elements(By.CSS_SELECTOR,'td')])\n",
    "\n",
    "\n",
    "#update dataframe with header \n",
    "df = pd.DataFrame(df, columns = [\"Organization Name\", \"NY Reg #\", \"EIN\" ,\"Registrant Type\",\"City\",\"State\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bucket does not exist\n",
      "webscraperm10 bucket has been created on AWS S3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# # Set the name of the bucket to create\n",
    "bucket_name = 'webscraper_10'\n",
    "\n",
    "aws_s3_client = boto3.client('s3',\n",
    "          aws_access_key_id = 'enter your access key id here',\n",
    "          aws_secret_access_key = 'enter your secret access key here'\n",
    "\n",
    "#Below commented code has the access key to my AWS account. \n",
    "response = aws_s3_client.list_buckets()\n",
    "bucket_exist = False\n",
    "\n",
    "for bucket in response['Buckets']:\n",
    "    if bucket['Name'] == bucket_name:\n",
    "        bucket_exist = True\n",
    "        break\n",
    "\n",
    "if bucket_exist:\n",
    "    print(\"The bucket exists\")\n",
    "else:\n",
    "    print(\"The bucket does not exist\")\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "if not bucket_exist:\n",
    "    try:\n",
    "        aws_s3_client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"{bucket_name} bucket has been created on AWS S3\")\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "        print(f\"{bucket_name} cannot be created on S3\")\n",
    "    except:\n",
    "        print(f\"{bucket_name} cannot be created on S3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Data\n",
      "Data uploaded successfully \n",
      "Successfull uploaded file to location:s3:/NAME_OF_YOUR_S3_BUCKET/charities_bureau_scrape_20230412003147.csv\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "def upload_s3(df,i):\n",
    "    global aws_s3_client,bucket_name\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer,header=True,line_terminator='\\n')\n",
    "    csv_buffer.seek(0)\n",
    "    aws_s3_client.put_object(Bucket=bucket_name,Body=csv_buffer.getvalue(),Key=i)\n",
    "\n",
    "# Checking if the bucket exists\n",
    "response = aws_s3_client.list_buckets()\n",
    "buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "if bucket_name not in buckets:\n",
    "    print(f\"{bucket_name} bucket does not exist.\")\n",
    "else:\n",
    "    print(\"Uploading Data\")\n",
    "    upload_s3(df, 'scrap_data.csv')\n",
    "    print(\"Data uploaded successfully \")\n",
    "\n",
    "filename= 'charities_bureau_scrape_' #name of your group\n",
    "datetime = time.strftime(\"%Y%m%d%H%M%S\") #timestamp\n",
    "filenames3 = \"%s%s.csv\"%(filename,datetime) #name of the filepath and csv file\n",
    "\n",
    "#print success message\n",
    "print(\"Successfull uploaded file to location:\"+str(filenames3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
