{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda93076",
   "metadata": {},
   "source": [
    "# Leveraging Advanced Data Acquisition Techniques for Real-time Twitter Analysis and AWS S3 Integration\n",
    "\n",
    "In an era dominated by big data and rapid information exchange, obtaining real-time data from social media platforms like Twitter and managing it effectively can provide organizations with a competitive edge. This guide explores advanced data acquisition techniques using Twitter's API to fetch real-time tweets related to specific topics such as COVID-19. Additionally, it discusses the process of integrating this data into Amazon Web Services (AWS) S3 for efficient storage and management, enabling further analysis and accessibility.\n",
    "\n",
    "Detailed Explanation\n",
    "Twitter Data Acquisition\n",
    "The process begins by setting up authentication with Twitter's API, which allows access to a broad range of data points from tweets. By utilizing specific queries, such as searching for recent mentions of \"COVID-19,\" the system can fetch relevant tweets that provide insights into public sentiment, trends, and more. This method involves encoding client credentials, handling authentication responses, and structuring API requests to retrieve the desired data efficiently.\n",
    "\n",
    "Data Handling and Transformation\n",
    "Once the data is retrieved, it is essential to structure it into a usable format. The raw data from Twitter, which includes details like tweet text, user information, timestamps, and more, is converted into a structured format such as a DataFrame. This transformation makes it easier to manipulate, analyze, and store the data. DataFrames provide a tabular format that is familiar and accessible for data analysis tasks.\n",
    "\n",
    "Integration with AWS S3\n",
    "After structuring the data, the next step involves storing it in a secure and scalable environment. AWS S3 is chosen for its robustness and flexibility in handling large datasets. The guide covers creating a new S3 bucket if one does not already exist and configuring it to ensure data security and integrity. The structured data is then uploaded to the S3 bucket using a direct method from the DataFrame, which simplifies the process and reduces the potential for data transmission errors.\n",
    "\n",
    "Automation and Scalability\n",
    "To facilitate ongoing data analysis projects, the process is designed to be automated, allowing for continuous data collection and storage without manual intervention. This automation is crucial for tracking evolving discussions and trends over time, particularly for time-sensitive topics like pandemics or other global events.\n",
    "\n",
    "Practical Applications\n",
    "The setup detailed in this guide is particularly useful for data scientists, market researchers, and social media analysts who require access to real-time data for rapid decision-making and trend analysis. By leveraging Twitter's extensive data and AWS's scalable infrastructure, they can perform complex analyses to extract actionable insights and respond proactively to changes in public opinion or market conditions.\n",
    "\n",
    "In summary, this approach not only facilitates efficient data collection and management but also enhances the capability to perform advanced analytics on real-time data from one of the world's largest social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c13d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.26.119 requires botocore<1.30.0,>=1.29.119, but you have botocore 1.29.76 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in e:\\anaconda\\anaconda1\\lib\\site-packages (2023.4.0)\n",
      "Requirement already satisfied: aiobotocore~=2.5.0 in e:\\anaconda\\anaconda1\\lib\\site-packages (from s3fs) (2.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in e:\\anaconda\\anaconda1\\lib\\site-packages (from s3fs) (3.8.3)\n",
      "Requirement already satisfied: fsspec==2023.4.0 in e:\\anaconda\\anaconda1\\lib\\site-packages (from s3fs) (2023.4.0)\n",
      "Collecting botocore<1.29.77,>=1.29.76\n",
      "  Using cached botocore-1.29.76-py3-none-any.whl (10.4 MB)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiobotocore~=2.5.0->s3fs) (1.12.1)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiobotocore~=2.5.0->s3fs) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in e:\\anaconda\\anaconda1\\lib\\site-packages (from aioitertools>=0.5.1->aiobotocore~=2.5.0->s3fs) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in e:\\anaconda\\anaconda1\\lib\\site-packages (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in e:\\anaconda\\anaconda1\\lib\\site-packages (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (2.8.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in e:\\anaconda\\anaconda1\\lib\\site-packages (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\anaconda1\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in e:\\anaconda\\anaconda1\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.2)\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.119\n",
      "    Uninstalling botocore-1.29.119:\n",
      "      Successfully uninstalled botocore-1.29.119\n",
      "Successfully installed botocore-1.29.76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "!pip install s3fs\n",
    "import s3fs # documentation: https://s3fs.readthedocs.io/en/latest/\n",
    "import time\n",
    "import twitter_keys #this is a custom reference module to a package containing twitter keys\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "\n",
    "key_secret = '{}:{}'.format(twitter_keys.client_key, twitter_keys.client_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')\n",
    "\n",
    "#identify base url and oauth token path\n",
    "base_url = 'https://api.twitter.com/' #base url for authentication\n",
    "auth_url = '{}oauth2/token'.format(base_url)\n",
    "\n",
    "#share header information -- encoding is ascii\n",
    "auth_headers = {\n",
    "    'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "    'Content-Type': 'UTF-8 Credentials'\n",
    "}\n",
    "\n",
    "#pass clientcredentials\n",
    "auth_data = {\n",
    "    'grant_type': 'client_credentials'\n",
    "}\n",
    "\n",
    "#send authentication using requests - POST request\n",
    "auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)\n",
    "\n",
    "#check response status. 200 = OK\n",
    "auth_resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6bf1cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['token_type', 'access_token'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keys in data response are token_type (bearer) and access_token (your access token)\n",
    "print(auth_resp.json().keys())\n",
    "\n",
    "access_token = auth_resp.json()['access_token']\n",
    "\n",
    "\n",
    "search_headers = {\n",
    "    'Authorization': 'Bearer {}'.format(access_token)    \n",
    "}\n",
    "\n",
    "#enter search parameters for coronavirus example. This looks for \"covid-19\" in the 1000 most recent tweets\n",
    "query_params = {\n",
    "    'q': 'covid-19',\n",
    "    'result_type': 'recent',\n",
    "    'count': 100, #update here to get more/less than 100 returns\n",
    "    'lang': 'en' #filters by english language only\n",
    "}\n",
    "\n",
    "\n",
    "#identify search url path and save \n",
    "search_url = '{}1.1/search/tweets.json'.format(base_url)\n",
    "\n",
    "\n",
    "#run search using get request\n",
    "search_resp = requests.get(search_url, headers=search_headers, params=query_params)\n",
    "\n",
    "#check status code of GET request\n",
    "search_resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6341f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @crampell: Florida Surgeon General Joseph Ladapo personally altered a state-driven study about Covid-19 vaccines last year to suggest th…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print text from result to verify  \n",
    "twitter_data = search_resp.json()\n",
    "\n",
    "for x in twitter_data['statuses']:\n",
    "    print(x['text'] + '\\n')\n",
    "    break #prints after one iteration and stops, remove break to see all 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b48a4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>entities</th>\n",
       "      <th>metadata</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>lang</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>quoted_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue Apr 25 18:48:19 +0000 2023</td>\n",
       "      <td>1650934779720908801</td>\n",
       "      <td>1650934779720908801</td>\n",
       "      <td>RT @crampell: Florida Surgeon General Joseph L...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>1578</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at                   id               id_str  \\\n",
       "0  Tue Apr 25 18:48:19 +0000 2023  1650934779720908801  1650934779720908801   \n",
       "\n",
       "                                                text  truncated  \\\n",
       "0  RT @crampell: Florida Surgeon General Joseph L...      False   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "\n",
       "                                              source  in_reply_to_status_id  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...                    NaN   \n",
       "\n",
       "  in_reply_to_status_id_str  ...  retweet_count favorite_count favorited  \\\n",
       "0                      None  ...           1578              0     False   \n",
       "\n",
       "  retweeted lang quoted_status_id quoted_status_id_str extended_entities  \\\n",
       "0     False   en              NaN                  NaN               NaN   \n",
       "\n",
       "  possibly_sensitive  quoted_status  \n",
       "0                NaN            NaN  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move data into data frame \n",
    "df = pd.DataFrame(twitter_data['statuses'])\n",
    "\n",
    "# show one record to verify import \n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856eea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import aws_s3 #this is a custom reference module to a package containing aws keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53677d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(bucket_name, region=None):\n",
    "    s3 = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    if region is None or region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "\n",
    "    print(f\"S3 bucket '{bucket_name}' created in region '{region}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d735145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket 'information-arch-dengyi-liu-assignment-lab3' created in region 'us-east-1'.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'information'\n",
    "region = 'us-east-1'\n",
    "create_s3_bucket(bucket_name, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25748a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull uploaded file to location:s3://information-arch-dengyi-liu-assignment-lab3/twitter_api20230425144827.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# prepare csv file name   \n",
    "pathname = 's3://'#specify location of s3:/{my-bucket}/\n",
    "filename= 'twitter_api' #name of your group\n",
    "datetime = time.strftime(\"%Y%m%d%H%M%S\") #timestamp\n",
    "filenames3 = \"%s%s%s.csv\"%(pathname,filename,datetime) #name of the filepath and csv file\n",
    "\n",
    "#load file into s3. Pandas actually leverages boto to connect to s3 and can push the file directly into an s3 bucket\n",
    "df.to_csv(filenames3, header=True, line_terminator='\\n') \n",
    "\n",
    "#print success message\n",
    "print(\"Successfull uploaded file to location:\"+str(filenames3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
