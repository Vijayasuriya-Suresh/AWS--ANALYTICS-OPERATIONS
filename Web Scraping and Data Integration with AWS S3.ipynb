{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbe1de1",
   "metadata": {},
   "source": [
    "# Advanced Web Scraping and Data Integration with AWS S3\n",
    "\n",
    "In the rapidly advancing field of data science, the ability to efficiently scrape, compile, and manage large datasets from the web is a valuable skill. This guide delves into enhancing a basic web scraping script to automatically navigate through multiple pages of a website and gather all available data into a single structured format. Furthermore, it discusses the method of securely uploading the compiled dataset to Amazon S3 (Simple Storage Service), ensuring the data is both accessible and stored reliably for further analysis or reporting.\n",
    "\n",
    "Detailed Explanation\n",
    "Enhanced Web Scraping\n",
    "The process begins with modifying an existing web scraping script to handle pagination on a website. This involves using a browser automation tool to interact with the website, navigate through its pages, and collect data from each page. The data typically includes various fields displayed in a tabular format on the website, relevant to the user's needs.\n",
    "\n",
    "Data Compilation\n",
    "As the script scrapes each page, it stores the data in a temporary format which is then aggregated into a single, comprehensive dataframe. This dataframe serves as a central repository of all the collected data, allowing for easy manipulation and analysis.\n",
    "\n",
    "Secure Data Upload to AWS S3\n",
    "Once the data is compiled, the next step involves uploading it to a cloud storage solution, specifically Amazon S3. This section of the process includes checking for the existence of an S3 bucket (a basic storage unit in Amazon S3), creating one if it does not exist, and then securely transferring the compiled dataset into this bucket. Amazon S3 provides a robust platform for storing large amounts of data securely, with features that manage data availability, security, and compliance.\n",
    "\n",
    "Automating and Monitoring Uploads\n",
    "The final part of the script automates the upload process and confirms the successful storage of data. It ensures that data integrity is maintained during the transfer and provides a confirmation once the upload is complete. This automated process is crucial for handling large datasets or regular updates where manual execution would be impractical.\n",
    "\n",
    "Practical Applications\n",
    "The combination of advanced web scraping techniques and the integration with Amazon S3 forms a powerful toolset for data scientists and analysts. It allows them to automatically gather and store data from various web sources, making the data ready for analysis, machine learning models, or reporting. This approach is particularly useful for projects requiring regular updates from dynamic web sources, such as daily financial data, social media statistics, or charitable organization registries.\n",
    "\n",
    "This workflow not only saves time but also enhances the reliability and accessibility of the data, enabling more sophisticated and timely data analysis projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6dde45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organization Name</th>\n",
       "      <th>NY Reg #</th>\n",
       "      <th>EIN</th>\n",
       "      <th>Registrant Type</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Forever Captain Poodaman\" The Ahmad Butler Fo...</td>\n",
       "      <td>48-07-16</td>\n",
       "      <td>843800926</td>\n",
       "      <td>NFP</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"R\" S.U.C.C.E.S.S. Foundation Inc.</td>\n",
       "      <td>49-06-59</td>\n",
       "      <td>874012670</td>\n",
       "      <td>NFP</td>\n",
       "      <td>ROCHESTER</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Studio 5404\" Inc.</td>\n",
       "      <td>44-39-58</td>\n",
       "      <td>463180470</td>\n",
       "      <td>NFP</td>\n",
       "      <td>MASSAPAQUA</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"THEY ARE HAITIAN\" FUND, INC.</td>\n",
       "      <td>20-63-46</td>\n",
       "      <td>300170128</td>\n",
       "      <td>NFP</td>\n",
       "      <td>HUDSON</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Y\" Dive, Inc.</td>\n",
       "      <td>48-45-01</td>\n",
       "      <td>854252095</td>\n",
       "      <td>NFP</td>\n",
       "      <td>SAINT ALBANS</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>University of Virginia Health Foundtion</td>\n",
       "      <td>40-44-88</td>\n",
       "      <td>412097394</td>\n",
       "      <td>NFP</td>\n",
       "      <td>CHARLOTTESVILLE</td>\n",
       "      <td>VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Violin Player</td>\n",
       "      <td>41-40-19</td>\n",
       "      <td>270773158</td>\n",
       "      <td>NFP</td>\n",
       "      <td>EAST AMHERST</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>William A. Epps Community Center, Inc.</td>\n",
       "      <td>40-91-11</td>\n",
       "      <td>861074714</td>\n",
       "      <td>NFP</td>\n",
       "      <td>STATEN ISLAND</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>WORLD SOCIETY OF CZESTOCHOWA JEWS AND THEIR DE...</td>\n",
       "      <td>40-46-49</td>\n",
       "      <td>205101779</td>\n",
       "      <td>NFP</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Yum-O Organization, Inc.</td>\n",
       "      <td>40-50-07</td>\n",
       "      <td>208107545</td>\n",
       "      <td>NFP</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Organization Name  NY Reg #        EIN  \\\n",
       "0   \"Forever Captain Poodaman\" The Ahmad Butler Fo...  48-07-16  843800926   \n",
       "1                  \"R\" S.U.C.C.E.S.S. Foundation Inc.  49-06-59  874012670   \n",
       "2                                  \"Studio 5404\" Inc.  44-39-58  463180470   \n",
       "3                       \"THEY ARE HAITIAN\" FUND, INC.  20-63-46  300170128   \n",
       "4                                      \"Y\" Dive, Inc.  48-45-01  854252095   \n",
       "..                                                ...       ...        ...   \n",
       "95            University of Virginia Health Foundtion  40-44-88  412097394   \n",
       "96                                      Violin Player  41-40-19  270773158   \n",
       "97             William A. Epps Community Center, Inc.  40-91-11  861074714   \n",
       "98  WORLD SOCIETY OF CZESTOCHOWA JEWS AND THEIR DE...  40-46-49  205101779   \n",
       "99                           Yum-O Organization, Inc.  40-50-07  208107545   \n",
       "\n",
       "   Registrant Type             City State  \n",
       "0              NFP     PHILADELPHIA    PA  \n",
       "1              NFP        ROCHESTER    NY  \n",
       "2              NFP       MASSAPAQUA    NY  \n",
       "3              NFP           HUDSON    NY  \n",
       "4              NFP     SAINT ALBANS    NY  \n",
       "..             ...              ...   ...  \n",
       "95             NFP  CHARLOTTESVILLE    VA  \n",
       "96             NFP     EAST AMHERST    NY  \n",
       "97             NFP    STATEN ISLAND    NY  \n",
       "98             NFP         NEW YORK    NY  \n",
       "99             NFP         NEW YORK    NY  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Load modules\n",
    "import awscli\n",
    "import boto3\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# create a new Chrome driver service\n",
    "chrome_driver_service = webdriver.chrome.service.Service('C:/Users/RAI/Desktop/Information-Architectures/chromedriver.exe')\n",
    "\n",
    "# start the service\n",
    "chrome_driver_service.start()\n",
    "\n",
    "# create a new Chrome browser instance using the service\n",
    "browser = webdriver.Chrome(service=chrome_driver_service)\n",
    "\n",
    "# enter the URL path that needs to be accessed by webdriver\n",
    "browser.get('https://www.charitiesnys.com/RegistrySearch/search_charities.jsp')\n",
    "\n",
    "# identify xpath of location to select element\n",
    "inputElement = browser.find_element(By.XPATH, \"/html/body/div/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[2]/td[2]/input[1]\")\n",
    "inputElement.send_keys('0')\n",
    "inputElement1 = browser.find_element(By.XPATH, \"/html/body/div/div[2]/div/table/tbody/tr/td[2]/div/div/font/font/font/font/font/table/tbody/tr[4]/td/form/table/tbody/tr[10]/td/input[1]\").click()\n",
    "time.sleep(4)  # allow for the page to load by adding a sleep element\n",
    "\n",
    "# Identify the table to scrape\n",
    "table = browser.find_element(By.CSS_SELECTOR, 'table.Bordered')\n",
    "time.sleep(1)\n",
    "\n",
    "# Create empty dataframe\n",
    "df_list = []\n",
    "\n",
    "# Loop through pages of results\n",
    "while True:\n",
    "    # Loop through dataframe to export table\n",
    "    for row in table.find_elements(By.CSS_SELECTOR, 'tr'):\n",
    "        cols = [cell.text for cell in row.find_elements(By.CSS_SELECTOR, 'td')]\n",
    "        if len(cols) > 0:  # exclude empty rows\n",
    "            df_list.append({\"Organization Name\": cols[0], \"NY Reg #\": cols[1], \"EIN\": cols[2], \"Registrant Type\": cols[3], \"City\": cols[4], \"State\": cols[5]})\n",
    "\n",
    "    # Check if there is another page of results\n",
    "    next_button = browser.find_elements(By.XPATH, \"//a[contains(text(),'Next')]\")\n",
    "    if len(next_button) > 0:\n",
    "        next_button[0].click()\n",
    "        time.sleep(4)  # allow for the page to load by adding a sleep element\n",
    "        table = browser.find_element(By.CSS_SELECTOR, 'table.Bordered')\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "df = pd.concat([pd.DataFrame(x, index=[0]) for x in df_list], ignore_index=True)\n",
    "\n",
    "# Display the scraped data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29eb48e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bucket does not exist\n",
      "webscraperm10part2 bucket has been created on AWS S3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# # Set the name of the bucket to create\n",
    "bucket_name = 'webscraperm10part2'\n",
    "\n",
    "aws_s3_client = boto3.client('s3',\n",
    "          aws_access_key_id = 'enter your access key id here',\n",
    "          aws_secret_access_key = 'enter your secret access key here')\n",
    "\n",
    "#Below commented code has the access key to my AWS account. \n",
    "\n",
    "response = aws_s3_client.list_buckets()\n",
    "bucket_exist = False\n",
    "\n",
    "for bucket in response['Buckets']:\n",
    "    if bucket['Name'] == bucket_name:\n",
    "        bucket_exist = True\n",
    "        break\n",
    "\n",
    "if bucket_exist:\n",
    "    print(\"The bucket exists\")\n",
    "else:\n",
    "    print(\"The bucket does not exist\")\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "if not bucket_exist:\n",
    "    try:\n",
    "        aws_s3_client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"{bucket_name} bucket has been created on AWS S3\")\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "        print(f\"{bucket_name} cannot be created on S3\")\n",
    "    except:\n",
    "        print(f\"{bucket_name} cannot be created on S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5416871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Data\n",
      "Data uploaded successfully \n",
      "Successfull uploaded file to location:charities_bureau_scrape_20230412104525.csv\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "def upload_s3(df,i):\n",
    "    global aws_s3_client,bucket_name\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer,header=True,line_terminator='\\n')\n",
    "    csv_buffer.seek(0)\n",
    "    aws_s3_client.put_object(Bucket=bucket_name,Body=csv_buffer.getvalue(),Key=i)\n",
    "\n",
    "# Checking if the bucket exists\n",
    "response = aws_s3_client.list_buckets()\n",
    "buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "if bucket_name not in buckets:\n",
    "    print(f\"{bucket_name} bucket does not exist.\")\n",
    "else:\n",
    "    print(\"Uploading Data\")\n",
    "    upload_s3(df, 'scrap2_data.csv')\n",
    "    print(\"Data uploaded successfully \")\n",
    "\n",
    "filename= 'charities_bureau_scrape_' #name of your group\n",
    "datetime = time.strftime(\"%Y%m%d%H%M%S\") #timestamp\n",
    "filenames3 = \"%s%s.csv\"%(filename,datetime) #name of the filepath and csv file\n",
    "\n",
    "#print success message\n",
    "print(\"Successfull uploaded file to location:\"+str(filenames3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83526f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
