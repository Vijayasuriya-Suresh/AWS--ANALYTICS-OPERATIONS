{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78c825a",
   "metadata": {},
   "source": [
    "# Seamless Data Management with AWS S3 and Pandas in Python\n",
    "\n",
    "Introduction\n",
    "In the realm of data science and analytics, managing and processing large datasets efficiently is a fundamental requirement. AWS S3 (Simple Storage Service) provides a robust and scalable cloud storage solution that can be seamlessly integrated with Python, particularly using the Pandas library, to facilitate the handling of extensive datasets. This guide will demonstrate the step-by-step process of setting up an AWS S3 bucket, uploading a dataset, and subsequently loading this dataset into a Pandas DataFrame. This workflow is crucial for data analysts and scientists who need to manipulate large datasets quickly and efficiently without compromising on performance.\n",
    "\n",
    "Detailed Explanation\n",
    "Setting Up an AWS S3 Bucket\n",
    "To begin with, you need to set up an AWS S3 bucket where your datasets will be stored:\n",
    "\n",
    "Create an AWS Account: Register and log into your Amazon Web Services account.\n",
    "Navigate to S3: Go to the S3 service page within the AWS Management Console.\n",
    "Create a New Bucket: Click on 'Create bucket', provide a unique name (e.g., m4assingment), and select the region closest to your location to reduce latency and costs.\n",
    "Configure Options: Set permissions and other options as per your requirements. Ensure that your bucket has the appropriate access policies if the data needs to be accessed by different users or applications.\n",
    "Uploading Dataset to S3\n",
    "After setting up your bucket, the next step is uploading your dataset:\n",
    "\n",
    "Prepare Your Dataset: Ensure your dataset, such as creditcard.csv, is ready for upload.\n",
    "Upload File: Navigate to your newly created bucket in the S3 console, click 'Upload', and select your dataset file to upload.\n",
    "Loading Data into Pandas\n",
    "With your dataset uploaded, you can now load it directly into a Pandas DataFrame:\n",
    "\n",
    "Configure AWS SDK: Use boto3, the AWS SDK for Python, to interact with your S3 bucket. Configure it with your credentials.\n",
    "\n",
    "\n",
    "Fetch Data from S3: Retrieve the object using the get_object method by specifying the bucket name and the file key.\n",
    "\n",
    "Load Data into DataFrame: Convert the CSV data into a DataFrame using Pandas.\n",
    "\n",
    "\n",
    "By integrating AWS S3 with Python using Pandas and boto3, data scientists and analysts can efficiently manage and analyze large datasets. This approach not only optimizes the data processing workflows but also leverages the power of cloud storage, making it a scalable solution for handling data-driven projects. This method ensures that datasets are easily accessible and manipulable across various platforms and applications, fostering a more flexible and productive data analysis environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6841b",
   "metadata": {},
   "source": [
    "### Access - load data into Pandas: Load the data into a Pandas data frame from your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03189395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = 'enter your access key id here',\n",
    "    aws_secret_access_key = 'enter your secret access key here',\n",
    "\n",
    ")\n",
    "\n",
    "s3 = session.client('s3')\n",
    "\n",
    "response = s3.get_object(Bucket='m4assingment', Key='creditcard.csv')\n",
    "\n",
    "bucket_name = 'm4assingment'\n",
    "file_path= 'creditcard.csv'\n",
    "\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "data = response['Body'].read().decode('utf-8')\n",
    "df = pd.read_csv(StringIO(data))\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
